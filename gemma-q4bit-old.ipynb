{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, BitsAndBytesConfig\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "prompt = \"Will AI take over the world?\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)\n",
    "\n",
    "%load_ext memory_profiler\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "%memit outputs = model.generate(**inputs, do_sample=True, max_new_tokens=150)\n",
    "inference_time = time.time() - start_time\n",
    "print(\"Inference time:\", inference_time)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(result)\n",
    "\n",
    "m_name = 'gemma-q4bit'\n",
    "model.save_pretrained(m_name)\n",
    "tokenizer.save_pretrained(m_name)\n",
    "model.push_to_hub(m_name)\n",
    "tokenizer.push_to_hub(m_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm_eval --model hf --model_args pretrained=gemma-q4bit --tasks winogrande,arc_challenge --device cuda:0 --num_fewshot 1 --batch_size 4 --output_path ./eval_harness/gemma-q4bit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
