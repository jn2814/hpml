{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning gemma with LORA r32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8490e8cf056f4df2b9ba3ecb914fe29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/395 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1003b41ed345d98d50b311268ae76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9addd0f9cb51450295147136aaa97e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d13831379a24d898ef4052167d72dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905ee929cddd4827a53d8f0c759717d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93292ef91468462d86db1fc3f7a85c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114570a226334703a4b4e63909286c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9f69f4e26d429185332b01bf2783c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 9,846\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 39,223,296\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 12:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.099500</td>\n",
       "      <td>2.026574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.917800</td>\n",
       "      <td>1.953279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./results_qlora/checkpoint-50\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/jn2814/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/config.json\n",
      "Model config GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_qlora/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in ./results_qlora/checkpoint-50/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 518\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./results_qlora/checkpoint-100\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/jn2814/.cache/huggingface/hub/models--google--gemma-2b/snapshots/2ac59a5d7bf4e1425010f0d457dde7d146658953/config.json\n",
      "Model config GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_activation\": null,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_qlora/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_qlora/checkpoint-100/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.0086538696289065, metrics={'train_runtime': 757.6556, 'train_samples_per_second': 0.528, 'train_steps_per_second': 0.132, 'total_flos': 2092234534600704.0, 'train_loss': 2.0086538696289065, 'epoch': 0.04061738424045491})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# followed tutorial here\n",
    "# https://medium.com/@bnjmn_marie/googles-gemma-fine-tuning-quantization-and-inference-on-your-computer-83066b25791b\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_name = \"google/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\n",
    "\n",
    "# Tokenizer pre processing\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id =  tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Load in finetuning dataset\n",
    "ds = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "\n",
    "#Quantization config\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, quantization_config=bnb_config, device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# Needed for finetuning on custom dataset\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False \n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"./results_qlora\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=50,\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "        eval_steps=50,\n",
    "        max_steps=100,\n",
    "        warmup_steps=30,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=32,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=ds['train'],\n",
    "        eval_dataset=ds['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in gemma-lora-r32/config.json\n",
      "Configuration saved in gemma-lora-r32/generation_config.json\n",
      "Model weights saved in gemma-lora-r32/model.safetensors\n",
      "tokenizer config file saved in gemma-lora-r32/tokenizer_config.json\n",
      "Special tokens file saved in gemma-lora-r32/special_tokens_map.json\n",
      "Configuration saved in gemma-lora-r32/config.json\n",
      "Configuration saved in gemma-lora-r32/generation_config.json\n",
      "Model weights saved in gemma-lora-r32/model.safetensors\n",
      "Uploading the following files to jn2814/gemma-lora-r32: README.md,generation_config.json,config.json,model.safetensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb829ca3b23473f823cea71448b5bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5f745f67904e1591d4f7f1b0cad19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in gemma-lora-r32/tokenizer_config.json\n",
      "Special tokens file saved in gemma-lora-r32/special_tokens_map.json\n",
      "Uploading the following files to jn2814/gemma-lora-r32: tokenizer.json,README.md,tokenizer_config.json,tokenizer.model,special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9c28dbf3f74b42ad299a74bc0afd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3d20d7a1cb4fb68b1e3e43762101a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0305cd8f9f41d2b991a2fd063e5bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jn2814/gemma-lora-r32/commit/a6d6b1da75f4c605ee952f6a56ad9312f455de9d', commit_message='Upload tokenizer', commit_description='', oid='a6d6b1da75f4c605ee952f6a56ad9312f455de9d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_name = 'gemma-lora-r32'\n",
    "model.save_pretrained(m_name)\n",
    "tokenizer.save_pretrained(m_name)\n",
    "model.push_to_hub(m_name)\n",
    "tokenizer.push_to_hub(m_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-08:17:55:54,159 INFO     [__main__.py:254] Verbosity set to INFO\n",
      "2024-05-08:17:56:00,206 INFO     [__main__.py:341] Selected Tasks: ['arc_challenge', 'winogrande']\n",
      "2024-05-08:17:56:00,208 INFO     [evaluator.py:141] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-08:17:56:00,208 INFO     [evaluator.py:178] Initializing hf model, with arguments: {'pretrained': 'gemma-lora-r32'}\n",
      "2024-05-08:17:56:00,226 INFO     [huggingface.py:165] Using device 'cuda:0'\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Some weights of the model checkpoint at gemma-lora-r32 were not used when initializing GemmaForCausalLM: ['model.layers.0.mlp.down_proj.base_layer.weight', 'model.layers.0.mlp.down_proj.base_layer.weight.absmax', 'model.layers.0.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.0.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.base_layer.weight', 'model.layers.0.mlp.up_proj.base_layer.weight.absmax', 'model.layers.0.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.0.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.0.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.base_layer.weight', 'model.layers.1.mlp.down_proj.base_layer.weight.absmax', 'model.layers.1.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.1.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.1.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.1.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.1.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.1.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.base_layer.weight', 'model.layers.1.mlp.up_proj.base_layer.weight.absmax', 'model.layers.1.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.1.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.1.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.1.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.base_layer.weight', 'model.layers.10.mlp.down_proj.base_layer.weight.absmax', 'model.layers.10.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.10.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.base_layer.weight', 'model.layers.10.mlp.up_proj.base_layer.weight.absmax', 'model.layers.10.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.10.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.10.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.base_layer.weight', 'model.layers.11.mlp.down_proj.base_layer.weight.absmax', 'model.layers.11.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.11.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.base_layer.weight', 'model.layers.11.mlp.up_proj.base_layer.weight.absmax', 'model.layers.11.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.11.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.11.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.base_layer.weight', 'model.layers.12.mlp.down_proj.base_layer.weight.absmax', 'model.layers.12.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.12.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.base_layer.weight', 'model.layers.12.mlp.up_proj.base_layer.weight.absmax', 'model.layers.12.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.12.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.12.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.base_layer.weight', 'model.layers.13.mlp.down_proj.base_layer.weight.absmax', 'model.layers.13.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.13.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.base_layer.weight', 'model.layers.13.mlp.up_proj.base_layer.weight.absmax', 'model.layers.13.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.13.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.13.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.base_layer.weight', 'model.layers.14.mlp.down_proj.base_layer.weight.absmax', 'model.layers.14.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.14.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.base_layer.weight', 'model.layers.14.mlp.up_proj.base_layer.weight.absmax', 'model.layers.14.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.14.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.14.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.base_layer.weight', 'model.layers.15.mlp.down_proj.base_layer.weight.absmax', 'model.layers.15.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.15.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.base_layer.weight', 'model.layers.15.mlp.up_proj.base_layer.weight.absmax', 'model.layers.15.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.15.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.15.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.base_layer.weight', 'model.layers.16.mlp.down_proj.base_layer.weight.absmax', 'model.layers.16.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.16.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.base_layer.weight', 'model.layers.16.mlp.up_proj.base_layer.weight.absmax', 'model.layers.16.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.16.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.16.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.base_layer.weight', 'model.layers.17.mlp.down_proj.base_layer.weight.absmax', 'model.layers.17.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.17.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.base_layer.weight', 'model.layers.17.mlp.up_proj.base_layer.weight.absmax', 'model.layers.17.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.17.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.17.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.base_layer.weight', 'model.layers.2.mlp.down_proj.base_layer.weight.absmax', 'model.layers.2.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.2.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.base_layer.weight', 'model.layers.2.mlp.up_proj.base_layer.weight.absmax', 'model.layers.2.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.2.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.2.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.base_layer.weight', 'model.layers.3.mlp.down_proj.base_layer.weight.absmax', 'model.layers.3.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.3.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.base_layer.weight', 'model.layers.3.mlp.up_proj.base_layer.weight.absmax', 'model.layers.3.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.3.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.3.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.base_layer.weight', 'model.layers.4.mlp.down_proj.base_layer.weight.absmax', 'model.layers.4.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.4.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.base_layer.weight', 'model.layers.4.mlp.up_proj.base_layer.weight.absmax', 'model.layers.4.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.4.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.4.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.base_layer.weight', 'model.layers.5.mlp.down_proj.base_layer.weight.absmax', 'model.layers.5.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.5.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.base_layer.weight', 'model.layers.5.mlp.up_proj.base_layer.weight.absmax', 'model.layers.5.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.5.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.5.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.base_layer.weight', 'model.layers.6.mlp.down_proj.base_layer.weight.absmax', 'model.layers.6.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.6.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.base_layer.weight', 'model.layers.6.mlp.up_proj.base_layer.weight.absmax', 'model.layers.6.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.6.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.6.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.base_layer.weight', 'model.layers.7.mlp.down_proj.base_layer.weight.absmax', 'model.layers.7.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.7.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.base_layer.weight', 'model.layers.7.mlp.up_proj.base_layer.weight.absmax', 'model.layers.7.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.7.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.7.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.base_layer.weight', 'model.layers.8.mlp.down_proj.base_layer.weight.absmax', 'model.layers.8.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.8.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.base_layer.weight', 'model.layers.8.mlp.up_proj.base_layer.weight.absmax', 'model.layers.8.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.8.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.8.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.base_layer.weight', 'model.layers.9.mlp.down_proj.base_layer.weight.absmax', 'model.layers.9.mlp.down_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.down_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight.absmax', 'model.layers.9.mlp.gate_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.gate_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.base_layer.weight', 'model.layers.9.mlp.up_proj.base_layer.weight.absmax', 'model.layers.9.mlp.up_proj.base_layer.weight.nested_absmax', 'model.layers.9.mlp.up_proj.base_layer.weight.nested_quant_map', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_map', 'model.layers.9.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.k_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.k_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.o_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.o_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.nested_absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.nested_quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing GemmaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GemmaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GemmaForCausalLM were not initialized from the model checkpoint at gemma-lora-r32 and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-08:17:56:19,852 WARNING  [big_modeling.py:450] You shouldn't move a model that is dispatched using accelerate hooks.\n",
      "2024-05-08:17:56:20,530 INFO     [huggingface.py:277] Model type is 'gemma', a BOS token will be used as Gemma underperforms without it.\n",
      "2024-05-08:17:56:32,592 WARNING  [evaluator.py:240] Overwriting default num_fewshot of winogrande from None to 1\n",
      "2024-05-08:17:56:32,592 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
      "2024-05-08:17:56:32,592 WARNING  [evaluator.py:240] Overwriting default num_fewshot of arc_challenge from None to 1\n",
      "2024-05-08:17:56:32,592 INFO     [evaluator.py:245] Setting fewshot random generator seed to 1234\n",
      "2024-05-08:17:56:32,594 INFO     [task.py:398] Building contexts for winogrande on rank 0...\n",
      "100%|████████████████████████████████████| 1267/1267 [00:00<00:00, 39607.83it/s]\n",
      "2024-05-08:17:56:32,671 INFO     [task.py:398] Building contexts for arc_challenge on rank 0...\n",
      "100%|██████████████████████████████████████| 1172/1172 [00:05<00:00, 231.82it/s]\n",
      "2024-05-08:17:56:37,792 INFO     [evaluator.py:395] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████| 7221/7221 [09:36<00:00, 12.52it/s]\n",
      "2024-05-08:18:06:18,945 WARNING  [huggingface.py:1306] Failed to get model SHA for gemma-lora-r32 at revision main. Error: 404 Client Error. (Request ID: Root=1-663bbf1a-4d9391cb34c943493e5887ba;5c56a188-02a3-40a8-a205-e8d084d0d8dc)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/models/gemma-lora-r32/revision/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "2024-05-08:18:06:28,129 INFO     [evaluation_tracker.py:132] Saving results aggregated\n",
      "hf (pretrained=gemma-lora-r32), gen_kwargs: (None), limit: None, num_fewshot: 1, batch_size: 2\n",
      "|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|arc_challenge|      1|none  |     1|acc     |0.2201|±  |0.0121|\n",
      "|             |       |none  |     1|acc_norm|0.2602|±  |0.0128|\n",
      "|winogrande   |      1|none  |     1|acc     |0.4949|±  |0.0141|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf --model_args pretrained=gemma-lora-r32 --tasks winogrande,arc_challenge --device cuda:0 --num_fewshot 1 --batch_size 2 --output_path ./eval_harness/gemma-lora-r32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6871.66 MiB, increment: 2.63 MiB\n",
      "Inference time: 12.290580749511719\n",
      "Will AI take over the world?2020年11月10日 星期日，北京时间06:06:55\n",
      "\n",
      "As more technology companies continue to pour billions into research into human-like AIs, how could the coming generation of AI’s impact on the course of human history play out?\n",
      "\n",
      "点击查看全文\n",
      "\n",
      "What are AI chips?  What are the benefits of AI chips? And how do AI chips work? As you know, AI (artificial intelligence) has been around for decades now; still the hype around AI is on an all time high. The more time that we all spend in social media, the more we hear about AI. But the more we wonder about what exactly are AI chips. And what are the\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "import time\n",
    "\n",
    "prompt = \"Will AI take over the world?\"\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "%memit outputs = model.generate(**inputs, do_sample=True, max_new_tokens=150)\n",
    "inference_time = time.time() - start_time\n",
    "print(\"Inference time:\", inference_time)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
